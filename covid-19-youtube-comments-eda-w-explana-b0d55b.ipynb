{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nIn this notebook, I will conduct **exploratory data analysis** for the [COVID-19 YouTube Comments dataset](https://www.kaggle.com/seungguini/youtube-comments-for-covid19-related-videos).\n\n1. [Data Preparation](#data-preparation)\n2. [Exploratory Data Analysis](#exploratory-data-analysis)\n    - Removing duplicate comments\n    - Word counts and character counts\n3. [Processing Text Data](#processing-text-data)\n    - Removing non-English comments\n    - Removing URLs, time-stamps, user-handles\n    - Extracting contractions, tokenization\n    - Lower-case, punctuation, and stop words\n    - Lemmatization","metadata":{}},{"cell_type":"markdown","source":"## 1. Data Preparation\n\nIn this notebook, we will work with the YouTube comment data provided [here](https://www.kaggle.com/seungguini/youtube-comments-for-covid19-related-videos). Let's take a look at our available files.","metadata":{}},{"cell_type":"code","source":"import time\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Text processing imports\n!pip install contractions langid nltk==3.2.4\n\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nimport string\nimport contractions\nimport langid\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\n\n# Wordcloud imports\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\n\n# ML models\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport datetime\n\n# Sentiment Analysis\n!pip install vaderSentiment\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-01T07:59:39.130116Z","iopub.execute_input":"2021-08-01T07:59:39.130554Z","iopub.status.idle":"2021-08-01T08:00:02.536755Z","shell.execute_reply.started":"2021-08-01T07:59:39.13046Z","shell.execute_reply":"2021-08-01T08:00:02.535577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initial Observations\nSeveral important observations from the dataset description and preview \n- Our dataset has six files, each in the form of `youtube_comments_query.csv`. The term at the end of each .csv file stands for the query used to request videos.\n- According to the dataset description, each _row_ represents a _comment_\n- Approximately 50 videos were scraped for each query, while approximately 100 comments were scraped for each video. This leads to roughly 5000 comments in total, per query.\n\nFirst off, we will examine the `youtube_comments_coronavirus.csv` dataset.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/youtube-comments-for-covid19-related-videos/covid_2021_1.csv')\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:00:05.695957Z","iopub.execute_input":"2021-08-01T08:00:05.696324Z","iopub.status.idle":"2021-08-01T08:00:06.24805Z","shell.execute_reply.started":"2021-08-01T08:00:05.696294Z","shell.execute_reply":"2021-08-01T08:00:06.247033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The query value seems to repeat itself for the values.\n- Each **row** represents a **comment**.\n\n## 2. Exploratory Data Analysis\n### Data content\nLet's take a look at the shape and makeup of our dataset.","metadata":{"trusted":true}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:00:17.967317Z","iopub.execute_input":"2021-08-01T08:00:17.967681Z","iopub.status.idle":"2021-08-01T08:00:17.973851Z","shell.execute_reply.started":"2021-08-01T08:00:17.967651Z","shell.execute_reply":"2021-08-01T08:00:17.97304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our data has 13 columns (aka. features), and 41,588 rows (comments).","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:00:40.219258Z","iopub.execute_input":"2021-08-01T08:00:40.219617Z","iopub.status.idle":"2021-08-01T08:00:40.28105Z","shell.execute_reply.started":"2021-08-01T08:00:40.219587Z","shell.execute_reply":"2021-08-01T08:00:40.279452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Our `Non-Null Count` for each feature is the same as the number of comments (4200), which means we aren't missing values.\n- 5 of our features are int64 values, while 8 of them are objects.\n\nLet's see what makes up our 8 `object` features:","metadata":{}},{"cell_type":"code","source":"for column in df.columns:\n    print(column, \":\", type(df[column][0]))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:01:05.04176Z","iopub.execute_input":"2021-08-01T08:01:05.042146Z","iopub.status.idle":"2021-08-01T08:01:05.051017Z","shell.execute_reply.started":"2021-08-01T08:01:05.042114Z","shell.execute_reply":"2021-08-01T08:01:05.050128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of our 8 `object` features contains `string` values.\n\nNow let's take a look at each feature. First, we'll create a general statistical overview of our data with the `pd.describe()` method\n\n> **_NOTE:_**  `pd.describe` displays statistics only for _numerical features_ by default. `.round(1)` rounds our statistical values to one decimal point, for better readability\n\n > **_See Also:_**  _We'll extract statistical observations for the text data later (through attribtues such as word and character counts)._","metadata":{}},{"cell_type":"code","source":"df.describe().round(1)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:01:34.724687Z","iopub.execute_input":"2021-08-01T08:01:34.725046Z","iopub.status.idle":"2021-08-01T08:01:34.767707Z","shell.execute_reply.started":"2021-08-01T08:01:34.725017Z","shell.execute_reply":"2021-08-01T08:01:34.766553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unique Values\n\nGreat! Let's also observe the number of unique values for each feature, with `df.nunique()`","metadata":{}},{"cell_type":"code","source":"df.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:01:41.697426Z","iopub.execute_input":"2021-08-01T08:01:41.698Z","iopub.status.idle":"2021-08-01T08:01:41.829563Z","shell.execute_reply.started":"2021-08-01T08:01:41.697948Z","shell.execute_reply":"2021-08-01T08:01:41.828218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The **number of titles (892)**, the **number of urls (895)**, and the **number of upload dates (894)** are all different. This indicates possible _duplicate values_ in our video data.\n- The **number of comments texts (40094)** and the **number of comment dates (41209)** are different. This indicates possible _duplicate values_ in our comments data as well.\n- The comments are **not** double counted, since there are 4,200 unique `comment_date` values. Thus, our dataset may contain spammed comments (repeated comments of the same text), or perhaps the same author is posting multiple comments in the same video.\n\nWe can explore further by examining frequency counts for the unique features. First, the frequency count for unique comments:","metadata":{}},{"cell_type":"code","source":"df['comment_text'].value_counts()[:10]","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:02:41.906335Z","iopub.execute_input":"2021-08-01T08:02:41.906725Z","iopub.status.idle":"2021-08-01T08:02:41.951783Z","shell.execute_reply.started":"2021-08-01T08:02:41.90669Z","shell.execute_reply":"2021-08-01T08:02:41.950675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like we have some repeated comments in our data. This could potentially be a red flag for biased data (i.e. spam comments), so let's check the number of repeated comments:","metadata":{}},{"cell_type":"code","source":"# Use boolean indexing to find repeated comments\nrepeats = df[df.groupby('comment_text')['comment_text'].transform('size') > 1]\nprint(\"number of repeats:\", len(repeats))\nprint(\"percentage of repeats:\",np.round(len(repeats)/len(df) * 100, 1), \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:03:12.537547Z","iopub.execute_input":"2021-08-01T08:03:12.537946Z","iopub.status.idle":"2021-08-01T08:03:12.669325Z","shell.execute_reply.started":"2021-08-01T08:03:12.537898Z","shell.execute_reply":"2021-08-01T08:03:12.66807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5% of our data have repeated comments, which could cause bias later on in our analysis.\n\n> _**NOTE**_ : We eliminate the repeated comments [here](#removing-duplicate-comments)\n","metadata":{}},{"cell_type":"markdown","source":"### Analyzing MetaData","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-01T08:03:18.321337Z","iopub.execute_input":"2021-08-01T08:03:18.321679Z","iopub.status.idle":"2021-08-01T08:03:18.329207Z","shell.execute_reply.started":"2021-08-01T08:03:18.32165Z","shell.execute_reply":"2021-08-01T08:03:18.327869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will first analyze four video metadata - `views`, `likes`, `dislikes`, and `comment_count`. As we've observed earlier, each row on the dataset represents a comment. Since there are approximately 50 comments per videos, the video metadata are repeated, as shown below:","metadata":{}},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the `title`, `views`, `likes`, and `dislikes` are repeated, because the first three comments are from the same video. This repetition can cause skew in our data. Fortunately, we can easily create a new dataframe with **video metadata** only with `df.drop_duplicates()`","metadata":{}},{"cell_type":"code","source":"df_videos = df.drop_duplicates(subset=['title'])\nprint(df_videos.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've removed our duplicate comments, leaving one representative comment (row) for each video. `youtube-comments-coronavirus` has 42 total videos.\n\nLet's visualize the distribution of the four video features:","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize=(10, 6))\n\nax[0][0].set(xlabel = 'likes', title='LIKES DISTRIBUTION')\nsns.histplot(ax=ax[0][0], x='dislikes', data=df_videos, kde=True, color='tab:orange')\n\nax[0][1].set(xlabel = 'dislikes', title='DISLIKES DISTRIBUTION')\nsns.histplot(ax=ax[0][1], x='views', data=df_videos, kde=True, color='tab:green')\n\nax[1][0].set(xlabel = 'views', title='VIEWS DISTRIBUTION')\nsns.histplot(ax=ax[1][0], x='views', data=df_videos, kde=True, color='tab:blue')\n\nax[1][1].set(xlabel = 'no. of comments', title='NO. OF COMMENTS DISTRIBUTION')\nsns.histplot(ax=ax[1][1], x='comment_count', data=df_videos, kde=True, color='tab:blue')\n\n# Formatting for scientific notations\nfor i in range(0,2):\n    for j in range(0,2):\n        ax[i][j].get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n        ax[i][j].get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n        ax[i][j].tick_params('x', labelrotation=30)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The histograms have a similar distribution shape, with a skewed tail to the right\n- The histograms have outliers on the right as well\n\nLet's checkout the **quantiles** for our features","metadata":{}},{"cell_type":"code","source":"print(\"Likes quantiles\")\nprint(df_videos['likes'].quantile([.01,.25,.5,.75,.99]))\nprint(\"\")\nprint(\"Dislikes quantiles\")\nprint(df_videos['dislikes'].quantile([.01,.25,.5,.75,.99]))\nprint(\"\")\nprint(\"Views quantiles\")\nprint(df_videos['views'].quantile([.01,.25,.5,.75,.99]))\nprint(\"No. of comments\")\nprint(df_videos['comment_count'].quantile([.01,.25,.5,.75,.99]))\nprint(\"\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's observe the correlation for the following relationships:\n- `views` on `likes`\n- `views` on `dislikes`\n- `views` on `comment_count`\n- `likes` on `dislikes`\n- `likes` on `comment_count`\n- `dislikes` on `comment_count`","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3,2, figsize=(9, 9))\nsns.scatterplot(ax=ax[0][0], x=\"views\", y=\"likes\", data=df_videos, color='tab:red')\nax[0][0].set(title='VIEWS V. LIKES')\n\nsns.scatterplot(ax=ax[1][0], x=\"views\", y=\"dislikes\", data=df_videos, color='tab:orange')\nax[1][0].set(title='VIEWS V. DISLIKES')\n\nsns.scatterplot(ax=ax[2][0], x=\"views\", y=\"comment_count\", data=df_videos, color='tab:green')\nax[2][0].set(ylabel='no. of comments', title='VIEWS V. NO. OF COMMENTS')\n\nsns.scatterplot(ax=ax[0][1], x=\"likes\", y=\"dislikes\", data=df_videos, color='tab:olive')\nax[0][1].set(title='LIKES V. DISLIKES')\n\nsns.scatterplot(ax=ax[1][1], x=\"likes\", y=\"comment_count\", data=df_videos, color='tab:blue')\nax[1][1].set(ylabel='no. of comments', title='LIKES V. NO. OF COMMENTS')\n\nsns.scatterplot(ax=ax[2][1], x=\"dislikes\", y=\"comment_count\", data=df_videos, color='tab:cyan')\nax[2][1].set(ylabel='no. of comments', title='DISLIKES V. NO. OF COMMENTS')\n\n# Formatting for scientific notations\nfor i in range(0,3):\n    for j in range(0,2):\n        ax[i][j].get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n        ax[i][j].get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n        ax[i][j].tick_params('x', labelrotation=30)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `likes`, `dislikes`, and `comment_count` all have a positive correlation with `views`\n- This is understandable, since the more views expose the video to a larger chance of having likes, dislikes, and comments\n- We can see that our **outlier videos** have _roughly 55,000,000 views_ and _75,000,000 views_.\n\n### Additional Features\nAs we observed, higher `views` tended to have higher `likes`, `dislikes`,  and `no. of comments`.\nLet's conduct a deeper analysis into these variables by creating a `like_ratio`, `dislike_ratio`, and `comments_ratio` - all three based on `views`","metadata":{}},{"cell_type":"code","source":"df_videos['likes_ratio'] = df_videos['likes'] / df_videos['views'] * 100\ndf_videos['dislikes_ratio'] = df_videos['views'] / df_videos['views'] * 100\ndf_videos['comments_ratio'] = df_videos['comment_count'] / df_videos['views'] * 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's visualize the distributions of our ratio!","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (9,6))\n\ng1 = sns.displot(df_videos['dislikes_ratio'], color='red', kind='kde', label=\"Dislike\")\ng1 = sns.displot(df_videos['likes_ratio'], color='green', kind='kde',label=\"Like\")\ng1 = sns.displot(df_videos['comments_ratio'], color='blue', kind='kde',label=\"Comment\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['upload_date']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing duplicate comments\n\nIn our Exploratory Data Analysis, we found some repeated comments in our data. These comments could cause potential bias as we continue our analysis and modeling. No worries! We can easily remove deleted comments with `drop duplicates`.\n\n> **_NOTE:_** set `inplace=True` to keep any changes made to our dataframe","metadata":{}},{"cell_type":"code","source":"df.drop_duplicates(subset=['comment_text'], inplace=True)\nprint(df['comment_text'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wonderful! All of our duplicate comments have been removed.\n\n### Generating word & character counts\nAs we can see from our output for `df.describe()` above, text data poses some challenges in creating statistical analysis. Unlike numerical data, text data does not explicitly give us figures to work with. However, we can manipulate our data to create useful features for analysis and modeling. This process is called **preprocessing**.\n\nThe two features we will be extracting are word and character counts. We will create four columns for our new features:\n- `title_wc` : word count for the video title\n- `title_cc` : character count for the video title\n- `comment_wc` : word count for the comment\n- `comment_cc` : character count for the comment","metadata":{}},{"cell_type":"code","source":"def wordcount(text):\n    count = 0\n    for word in text.split():\n        count += 1\n    return count\n\ndf['title_wc'] = df['title'].apply(lambda title : wordcount(title))\ndf['comment_wc'] = df['comment_text'].apply(lambda comment : wordcount(comment))\ndf['title_cc'] = df['title'].apply(lambda title : len(title))\ndf['comment_cc'] = df['comment_text'].apply(lambda comment : len(comment))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Awesome! Now that we've created our word and text counts for both titles and comments, let's check out their distributions!","metadata":{"trusted":true}},{"cell_type":"code","source":"fig, ax = plt.subplots(2, figsize=(6, 9))\nsns.histplot(ax=ax[0], x=\"title_wc\", data=df, color='tab:red', kde=True, bins=100)\nax[0].set(title='TITLE WORD COUNT DISTRIBUTION')\n\nsns.histplot(ax=ax[1], x=\"title_cc\", data=df, color='tab:orange', kde=True)\nax[1].set(title='TITLE CHARACTER COUNT DISTRIBUTION')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(2, figsize=(6, 9))\nsns.histplot(ax=ax[0], x=\"comment_wc\", data=df, color='tab:red', kde=True, bins=100)\nax[0].set(title='COMMENT WORD COUNT DISTRIBUTION')\n\nsns.histplot(ax=ax[1], x=\"comment_cc\", data=df, color='tab:orange', kde=True)\nax[1].set(title='COMMENT CHARACTER COUNT DISTRIBUTION')\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try find some basic metrics about our newly added features! We'll use Panda's `.describe()` function used earlier :)","metadata":{}},{"cell_type":"code","source":"new_df = df[['title_wc','title_cc','comment_wc','comment_cc']]\nnew_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Processing Text Data\n\nSo far, we have observed our data's shape and general correlations. Now, we would like to perform deeper analysis on our text data (most common words, sentiment analysis, etc). However, computers (and their processors) are unable to understand text information the way humans can. In order to allow our program to properly analyze our data, we process our **titles** and **commments**.  ","metadata":{}},{"cell_type":"markdown","source":"### Removing URLs, time-stamps, user-handles\n> Some comments may hold URLs, time-stamps, or user-handles. We remove these using regular expressions","metadata":{}},{"cell_type":"code","source":"# Remove URLS\ndf['processed'] = df['comment_text'].apply(lambda comment : re.sub(r\"http\\S+\", \"\", comment))\n\n# Remove time-stamps\ndf['processed'] = df['processed'].apply(lambda comment : re.sub(r\"\\d+:\\d{2}\", \"\", comment))\n\n# Remove user-handles\ndf['processed'] = df['processed'].apply(lambda comment : re.sub(r\"@[^\\s]+\", \"\", comment))\n\n# Remove numbers\ndf['processed'] = df['processed'].apply(lambda comment : re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", comment))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing non-English comments\n\nFor this Notebook, we will work with only English comments. First, let's tag our comments with their language using the `langdetect` package:","metadata":{}},{"cell_type":"code","source":"# Function to detect the language\ndef detectLang(corpus) :\n  try:\n    lang = langid.classify(corpus)[0]\n    if lang == 'en':\n      return lang\n    else:\n      return None\n  except:\n    return None\n\n# Tag comment language\n%time df['lang'] = df['processed'].apply(lambda comment: detectLang(comment))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['lang'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Approximately 7000 of our comments are not English. Let's drop these values:","metadata":{}},{"cell_type":"code","source":"df['lang'].dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extracting contractions, tokenization\n\nFirst of all, we will extract contractions _(don't --> do not)_ by using the `contractions` package.\n![](http://)> _NOTE_ : `contractions.fix(word)` tokenizes each sentence into a list individual words, while expanded contraptions are placed within a single string. In order to properly tokenize, we append our words back into a comment, before tokenizing it again.\n> _EXAMPLE_ : `I don't care` --> `['I', 'do not', 'care`] --> `I do not care` --> `['I', 'do', 'not', 'care']`","metadata":{}},{"cell_type":"code","source":"%%time\ndf['no_contract'] = df['comment_text'].apply(lambda x: [contractions.fix(word) for word in x.split() ])\ndf['comment_str'] = [' '.join(map(str, l)) for l in df['no_contract']]\n\n# Tokenize the comments again\ndf['tokenized'] = df['comment_str'].apply(lambda x : word_tokenize(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lower-case, punctuation, and stop words\nNext, we will convert our text into lower case and remove all punctuation symbols. We will also remove stop words ('the', 'is', 'a', etc) using NLTK's `stopwords` module","metadata":{}},{"cell_type":"code","source":"%%time\n# Convert characters to lowercase\ndf['lower'] = df['tokenized'].apply(lambda x: [word.lower() for word in x])\n\n# Remove punctuations\npunct = string.punctuation\n# Make a list only if token is not a punctuation\ndf['processed'] = df['lower'].apply(lambda x: [word for word in x if word not in punct])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop unnecessary columns\ndf.drop(['lang', 'no_contract', 'comment_str', 'lower', 'tokenized'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Most Common Words\nNow that our comments are processed and tokenized, let's analyze the most commonly used words throughout our comments.\nWe visualize our top used words with a `WordCloud`.","metadata":{}},{"cell_type":"code","source":"# Define a function to plot the wordcloud\n\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(20, 10))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Format comments into one, large String for the wordcloud\ndf['processed_sentence'] = df['processed'].apply(lambda comment : \" \".join(comment))\n%time text = ' '.join(df['processed_sentence'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create YouTube logo mask\nmask = np.array(Image.open('../input/youtube-logo/youtube_social_square_white.png'))\n\n# Generate the wordcloud\nwordcloud = WordCloud(width = mask.shape[1], height = mask.shape[0], background_color='white', colormap='Reds', mask=mask, max_words=300, max_font_size=300)\nwordcloud.generate(text)\n\n# Plot\nplot_cloud(wordcloud)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. K-Means Clustering\n\nTo further examine what our comments are talking about, let's perform **K-means clustering** on our data. This will help divide our Tweets into specific clusters that could give us insight on the different topics / discussions in our dataset.\n\n---\n\n### TF-IDF Vectorization\n\nBefore we can run **K-Means Clustering**, we must first vectorize our dataset. We will use the **Term Frequency-Inverse Document Frequency (TF-IDF)** vectorization method on our dataset. More on TF-IDF can be found [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n\nThe implementation of **TF-IDF** is provided by `sklearn`:","metadata":{}},{"cell_type":"code","source":"df['processed_sentence']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ndata = df['processed_sentence']\n# data.head()\n\ntf_idf_vectorizor = TfidfVectorizer(max_features = 5000)\n%time tf_idf = tf_idf_vectorizor.fit_transform(data)\ntf_idf_norm = normalize(tf_idf)\ntf_idf_array = tf_idf_norm.toarray()\ntf_idf_df = pd.DataFrame(tf_idf_array, columns=tf_idf_vectorizor.get_feature_names())\ntf_idf_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf_df.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nprint(tf_idf_df.shape)\npca = PCA(0.95)\nreduced = pca.fit_transform(tf_idf_df)\nprint(reduced.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(pca.explained_variance_ratio_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* * ","metadata":{}},{"cell_type":"markdown","source":"### Optimizing K with the Elbow Method\n\nIn order to figure out which K-value (the number of clusters to create) is optimal, we use the [Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering))\n\nFirst, we run the K-means clusters with k ranging from 1 to 8. Then, we graph our score to find the point of inflection on our cluster-score graph:","metadata":{}},{"cell_type":"code","source":"%%time\n\nn_clusters = range(1, 7)\n\ninertia = []\nfor i in n_clusters:\n    kmeans = KMeans(n_clusters=i, max_iter=600, algorithm = 'auto')\n    kmeans.fit(reduced)\n    inertia.append(kmeans.inertia_)\n\nplt.plot(n_clusters, inertia)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Method')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Though our elbow graph is not the clearest, our score seems to only marginally increase when there are between _3 to 5 clusters_. We'll experiment with both these numbers.","metadata":{}},{"cell_type":"code","source":"%time kmc = KMeans(n_clusters=3, max_iter=600, algorithm = 'auto').fit(principalComponents)\npredicted_values = kmc.predict(principalComponents)\n\nplt.scatter(principalComponents[:, 0], principalComponents[:, 1], c=predicted_values, s=50, cmap='viridis')\n\ncenters = kmc.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n    labels = np.unique(prediction)\n    dfs = []\n    for label in labels:\n        id_temp = np.where(prediction==label) # indices for each cluster\n        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n        features = tf_idf_vectorizor.get_feature_names()\n        best_features = [(features[i], x_means[i]) for i in sorted_means]\n        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n        dfs.append(df)\n    return dfs\n\ndef plotWords(dfs, n_feats):\n    plt.figure(figsize=(8, 4))\n    for i in range(0, len(dfs)):\n        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_feats = 20\ndfs = get_top_features_cluster(principalComponents, predicted_values, n_feats)\nplotWords(dfs, 13)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Sentiment Analysis\nThere are many ways to perform sentiment analysis on our Tweets. For practice and comparison purposes, I will use several well known Sentiment Analysis models ([VADER](https://github.com/cjhutto/vaderSentiment), [TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html)), while also attempting to build my own models. To train my models, I will use the **1.6M Tweet Sentiment dataset**, available on [Kaggle](https://www.kaggle.com/kazanova/sentiment140).","metadata":{}},{"cell_type":"markdown","source":"### Sentiment classification using Logistic Regression\nWe will train a **logistic regression** from `sklearn` to create a model that predicts sentiments.\nBefore we dive into our models, it's important to clarify the **evaluation metrics** for our classification model.\n\n> _DISCLAIMER_ : \nThis [Medium article](https://towardsdatascience.com/sentiment-classification-with-logistic-regression-analyzing-yelp-reviews-3981678c3b44#c3b8) has been incredibly helpful in building the following model. Most of the concepts, explanations, and models are built off the article's contents. Please check it out if you are interested! \n\nFor our tasks, here are several evaluation metrics:\n1. **Precision** - True Positive/(True Positive + False Positive), meaning the proportion of points that model classify as positives are actually positives.\n2. **Recall** - True Positive/(True Positive + False Negative), meaning the proportion of actual positives that are correctly classified by the model.\n3. **F1 score** —the harmonic mean of precision and recall.\n\nWe will use the **F1 score** as the primary evaluation metric for our model","metadata":{}},{"cell_type":"markdown","source":"### Text Preprocessing\n\nFirst, let's import our dataset from [Kaggle](https://www.kaggle.com/kazanova/sentiment140):","metadata":{}},{"cell_type":"code","source":"df_sentiment = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv', encoding='latin', header=None)\ndf_sentiment.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our dataset is missing headers, so we will assign them:","metadata":{}},{"cell_type":"code","source":"df_sentiment.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\ndf_sentiment.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we're using this dataset to create a Sentiment Analysis model, let's drop the unnecessary columns.","metadata":{}},{"cell_type":"code","source":"df_sentiment.drop(['id', 'date', 'query', 'user_id'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to our dataset description, this is how the `sentiment` column has been anotated:\n- `4` : positive\n- `2` : neutral\n- `0` : negative\nWe will convert the numerical values into `positive`, `negative`, or `neutral`:","metadata":{}},{"cell_type":"code","source":"annotation = {\n    4: 'positive',\n    2: 'neutral',\n    0: 'negative'\n}\ndf_sentiment['sentiment'] = df_sentiment['sentiment'].apply(lambda sentiment : annotation[sentiment])\ndf_sentiment.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the distribution of sentiments in our dataset","metadata":{}},{"cell_type":"code","source":"val_count = df_sentiment.sentiment.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"Sentiment Data Distribution\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thankfullly, our dataset is without much skewness. Just like we did for the **YouTube Comments Dataset**, let's perform **text preprocessing** on our new dataset. We will use similar methods as above:","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\ndef process_sentence(sentence):\n    words = []\n    punct = string.punctuation\n    \n    # Remove URLS\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n\n    # Remove time-stamps\n    sentence = re.sub(r\"\\d+:\\d{2}\", \"\", sentence)\n\n    # Remove user-handles\n    sentence = re.sub(r\"@[^\\s]+\", \"\", sentence)\n    \n    for word in sentence.split():\n        words.extend(contractions.fix(word).split())\n    words = [word.lower() for word in words]\n    words = [word for word in words if word not in punct]\n    return words\n\ndf_sentiment['processed'] = df_sentiment['text'].apply(lambda sentence : process_sentence(sentence))\nprint(\"Time taken to convert preprocess text data: \", round((time.time() - start_time)/60, 2), \" mins\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sentiment.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop unnecessary columns\ndf_sentiment.drop(['no_contract', 'comment_str', 'tokenized', 'lower'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Count Vectorization\nBefore we train our model, we must first convert our text input into vectors. \nWe will use the `CountVectorizer` from `sklearn` to extract features from our words.\n\nFirst, let's split our dataset into train and test data for our models:","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(df_sentiment, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's create a binary feature representation of our words:","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\ncv = CountVectorizer(binary=True, min_df = 10, max_df = 0.95)\ncv.fit_transform(train['text'].values)\ntrain_feature_set=cv.transform(train['text'].values)\ntest_feature_set=cv.transform(test['text'].values)\nprint(\"Time taken to convert text input into feature vector: \", round((time.time() - start_time)/60, 2), \" mins\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! We've created a `CountVectorizer` matrix, which simply creates a matrix counting the occurances of each vocabulary in every Tweet.\n\nNow, we can set each Tweet's sentiment as our target value:","metadata":{}},{"cell_type":"code","source":"y_train = train['sentiment'].values\ny_test = test['sentiment'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training our Logistic Regression Model for Sentiment Analysis\n\nAwesome! Now that we've feature engineered our text data with `CountVectorizer` and created our target variables, let's train our **logistic regression** model:","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\nlr = LogisticRegression(solver = 'liblinear', random_state = 42, max_iter=1000)\nlr.fit(train_feature_set,y_train)\ny_pred = lr.predict(test_feature_set)\nprint(\"Time taken to train model and make predictions: \", round((time.time() - start_time)/60, 2), \" mins\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We analyze our model's accuracy and F1 score:","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(\"Train Score\", lr.score(train_feature_set, y_train))\nprint(\"Test Score\", lr.score(test_feature_set, y_test))\nprint(\"Test Accuracy: \",round(metrics.accuracy_score(y_test,y_pred),3))\nprint(\"F1: \",round(metrics.f1_score(y_test, y_pred, pos_label=\"negative\"),3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using our model to predict sentiments for COVID YouTube Comments","metadata":{}},{"cell_type":"code","source":"# Initialize VADER sentiment analyzer\nanalyzer = SentimentIntensityAnalyzer()\n\nstart_time = time.time()\ndf['sentiment'] = df['processed_sentence'].apply(lambda comment : analyzer.polarity_scores(comment)['compound'])\nprint(\"Time taken to create sentiment analysis with VADER: \", round((time.time() - start_time)/60, 2), \" mins\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}